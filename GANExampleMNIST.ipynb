{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Introduction\n",
    "#### The notebook presents a simple bare-minimum example of Deep Convolutional Generative Adverserial Network (or DCGAN). \n",
    "\n",
    "Note: The focus is on readability rather than functionality. None of the design choices: network, parameters, or training approach may be optimal. Making this work on larger datasets or more complicated images may require some tweaks. \n",
    "\n",
    "## Goal\n",
    "We want to learn an image generator network _G_. It takes a vector of random numbers and spits out an image. In this specific example, we want to generate simple (28x28 pixel) images of handwritten digits.\n",
    "\n",
    "<img src=\"./imgs/generator.svg\" width=\"475\">\n",
    "\n",
    "## Why GAN?\n",
    "The GAN framework allows us to learn the network G using another network which can distinguish between a real/fake image, called a disciminator network D. They work as adversaries to train each other. Hence, the name Generative Adversearial Networks.\n",
    "\n",
    "<img src=\"./imgs/discriminator.svg\" width=\"400\">\n",
    "\n",
    "## Training \n",
    "Training a GAN is different from a vanilla deep network. In each training iteration there are two steps. Step 1 updates the parameters of D using the real images and the images generated from G. And, step 2 updates the parameters of G using the feedback from the output of D. This will become more concrete using the e\n",
    "<img src=\"./imgs/LearningGAN.svg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(mnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples from MNIST dataset.\n",
      "Image size: 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle= True)\n",
    "\n",
    "# Get a sample batch.\n",
    "it = iter(data_loader)\n",
    "images, _ = it.next()\n",
    "\n",
    "# Get an example batch.\n",
    "disp_img = images.squeeze().numpy()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 2))  \n",
    "\n",
    "# Display.\n",
    "print(\"Samples from MNIST dataset.\")\n",
    "print(\"Image size: {}x{}\".format(disp_img.shape[1], disp_img.shape[2]))\n",
    "for ii in range(disp_img.shape[0]):\n",
    "    fig.add_subplot(1, 10, ii+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(disp_img[ii, :, :]), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define a Generator (G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator architecture\n",
    "<img src=\"./imgs/networkG.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 4*4*128)\n",
    "        self.deconv1 = nn.Sequential(nn.ConvTranspose2d(128, 64, 3, stride = 2, padding = 1),\n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.ReLU())\n",
    "        self.deconv2 = nn.Sequential(nn.ConvTranspose2d(64, 32, 4, stride = 2, padding = 1),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.ReLU())\n",
    "        self.deconv3 = nn.Sequential(nn.ConvTranspose2d(32, 1, 4, stride = 2, padding = 1),\n",
    "                                     nn.Tanh())\n",
    "                       \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.shape[0], 128, 4, 4)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        return x\n",
    "    \n",
    "'''\n",
    "Bring the Generator output from [-1, +1] -> [0, 1]\n",
    "'''\n",
    "def out2img(x):\n",
    "    return (x + 1.0)/2.0\n",
    "\n",
    "'''\n",
    "Preprocess the real image to input to the discriminator [-1, +1]\n",
    "'''\n",
    "def img2inp(x):\n",
    "    return (x - 0.5)*2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sample image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_size = 100 # Size of the random input vector\n",
    "G = Generator(z_size)\n",
    "\n",
    "# Generate a random vector and pass it through the generator.\n",
    "z = torch.randn(1, z_size)\n",
    "g_im = G(z)\n",
    "\n",
    "# Display.\n",
    "disp_img = out2img(g_im.detach().numpy().squeeze())\n",
    "print(\"Image size: {}x{}\".format(disp_img.shape[0], disp_img.shape[1]))\n",
    "plt.axis('off')\n",
    "plt.imshow(disp_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define a Descriminator (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/networkD.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 4, stride = 2, padding=1),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 4, stride = 2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, stride = 2, padding=1),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.fc = nn.Linear(128*4*4, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available(): \n",
    "    use_cuda = True\n",
    "    print(\"Using the GPU.\")\n",
    "else:\n",
    "    print(\"Cuda is not availabe, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(pred):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    expected_output = torch.ones(pred.shape[0], 1)\n",
    "    if use_cuda:\n",
    "        expected_output = expected_output.cuda()\n",
    "    return criterion(pred, expected_output)\n",
    "\n",
    "def fake_loss(pred):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    expected_output = torch.zeros(pred.shape[0], 1)\n",
    "    if use_cuda:\n",
    "        expected_output = expected_output.cuda()\n",
    "    return criterion(pred, expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate num_imgs x h x w tensor into a single image.\n",
    "def concat_imgs(np_imgs): \n",
    "    num_imgs = np_imgs.shape[0]\n",
    "    h = np_imgs.shape[1]\n",
    "    w = np_imgs.shape[2]\n",
    "    out = np.zeros((h, num_imgs*w))\n",
    "    for i in range(num_imgs):\n",
    "        out[:, i*w:(i+1)*w] = np_imgs[i,:,:]\n",
    "    return out        \n",
    "\n",
    "\n",
    "def train_GAN(G, D, optimizer_g, optimizer_d, input_size, batch_size = 32, num_epochs = 30):\n",
    "    sample_size = 10\n",
    "    z_fixed = torch.randn(sample_size, input_size)\n",
    "    if use_cuda:\n",
    "        z_fixed = z_fixed.cuda()\n",
    "        \n",
    "    # GET THE DATA\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = batch_size, shuffle= True)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        d_losses = 0.0\n",
    "        g_losses = 0.0        \n",
    "        # Iterate over the dataset.\n",
    "        for ii, (real_imgs, _) in enumerate(data_loader):\n",
    "            # Bring the input image pixels between -1 and 1\n",
    "            real_imgs = img2inp(real_imgs)   \n",
    "            if use_cuda:\n",
    "                real_imgs = real_imgs.cuda()\n",
    "                \n",
    "            # ======== TRAIN DESCRIMNATOR ===============\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            pred = D(real_imgs)\n",
    "            # D should learns to predict real image as real, i.e., label 1\n",
    "            real_loss_d = real_loss(pred)\n",
    "            \n",
    "            # Create fake examples.\n",
    "            z = torch.randn(batch_size, input_size)\n",
    "            if use_cuda:\n",
    "                z = z.cuda()\n",
    "            fake_imgs = G(z)    \n",
    "            pred = D(fake_imgs.detach())\n",
    "            # D should learns to predict the generated images as fake, i.e., label 0\n",
    "            fake_loss_d = fake_loss(pred) \n",
    "            \n",
    "            # Descriminator opt takes a step.\n",
    "            total_loss_d = real_loss_d + fake_loss_d\n",
    "            total_loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # ============= TRAIN GENERATOR =============\n",
    "            optimizer_g.zero_grad()\n",
    "            # Create more fake examples.\n",
    "            z = torch.randn(batch_size, input_size) \n",
    "            if use_cuda:\n",
    "                z = z.cuda()\n",
    "            fake_imgs = G(z)\n",
    "            pred = D(fake_imgs)     \n",
    "            # G tries to learn such that generated images pass as real.\n",
    "            loss_g = real_loss(pred) \n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            # Save losses\n",
    "            d_losses += total_loss_d.detach().cpu().item()\n",
    "            g_losses += loss_g.detach().cpu().item()\n",
    "            \n",
    "            sys.stdout.write(\"\\r Processing: {}/{}...\".format(ii+1, len(data_loader)))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        d_loss_epoch = d_losses/len(data_loader)\n",
    "        g_loss_epoch = g_losses/len(data_loader)\n",
    "        print(\"Epoch: {}, D loss: {}, G loss: {}\".format(e, d_loss_epoch, g_loss_epoch))\n",
    "        \n",
    "        # Save a few images as example.\n",
    "        G.eval()\n",
    "        out_imgs = G(z_fixed)\n",
    "        out_imgs = out2img(out_imgs) # Convert to right size.\n",
    "        disp_imgs = np.squeeze(out_imgs.detach().cpu().numpy())\n",
    "        combined_img = concat_imgs(disp_imgs)*255\n",
    "        combined_img = combined_img.astype(np.uint8)\n",
    "        img_to_save = Image.fromarray(combined_img)\n",
    "        img_to_save.save(\"./data/sample_output{}.png\".format(e))\n",
    "        \n",
    "        # Save the checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': e + 1,\n",
    "            'state_dict': G.state_dict()         \n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoint.pth')\n",
    "        G.train()\n",
    "    print(\"Done training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from file...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "G = Generator(z_size)\n",
    "D = Discriminator()\n",
    "if use_cuda:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n",
    "\n",
    "if os.path.exists('checkpoint.pth'):\n",
    "    print(\"Loading trained model from file...\")\n",
    "    G.load_state_dict(torch.load('checkpoint.pth')[\"state_dict\"])\n",
    "else:\n",
    "    print(\"Training...\")\n",
    "    # Train your own model.\n",
    "    lr = 0.0001\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "\n",
    "    optimg = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimd = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    train_GAN(G, D, optimg, optimd, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample image:  epoch 01\n",
    "<img src=\"./imgs/sample_output0.png\" width=\"800\">\n",
    "\n",
    "#### Sample images: epoch 30\n",
    "<img src=\"./imgs/sample_output29.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G.eval()\n",
    "z = torch.randn(1, z_size)\n",
    "if use_cuda:\n",
    "    z = z.cuda()\n",
    "G_out = G(z)\n",
    "img = np.squeeze(G_out.detach().cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(2, 2))  \n",
    "plt.axis('off')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Wonderful animations of convolution arithmatic: https://github.com/vdumoulin/conv_arithmetic\n",
    "2. A more detailed GAN example from torch developes: https://github.com/pytorch/examples/tree/master/dcgan\n",
    "3. How to train a GAN: tips and tricks: https://github.com/soumith/ganhacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
