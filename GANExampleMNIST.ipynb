{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Introduction\n",
    "#### The notebook presents a simple bare-minimum example of Generative Adverserial Network (or GAN). \n",
    "\n",
    "Note: The focus is on readability rather than functionality. None of the design choices: network, parameters, or training approach may be optimal. Making this work on larger datasets or more complicated images may require changes. \n",
    "\n",
    "## Goal\n",
    "We want to learn an image generator network _G_. It takes a vector of random numbers and spits out an image. In this specific example, we want to generate simple (28x28 pixel) images of handwritten digits. The GAN framework allows us to learn the network G using another network which concurrently learns to distinguish between a real/fake image, called a disciminator network D. They work as adversaries to train each other. Hence, the name Generative Adversearial Networks.\n",
    "\n",
    "<img src=\"./imgs/GD.png\" width=\"700\">\n",
    "\n",
    "\n",
    "## Training \n",
    "In each training iteration there are two steps. Step 1 updates the parameters of D using the real images and the images generated from G. And, step 2 updates the parameters of G using the feedback from the output of D. This will become more concrete using the example below.\n",
    "<img src=\"./imgs/LearningGAN.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(mnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples from MNIST dataset.\n",
      "Image size: 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle= True)\n",
    "\n",
    "# Get a sample batch.\n",
    "it = iter(data_loader)\n",
    "images, _ = it.next()\n",
    "\n",
    "# Get an example batch.\n",
    "disp_img = images.squeeze().numpy()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 2))  \n",
    "\n",
    "# Display.\n",
    "print(\"Samples from MNIST dataset.\")\n",
    "print(\"Image size: {}x{}\".format(disp_img.shape[1], disp_img.shape[2]))\n",
    "for ii in range(disp_img.shape[0]):\n",
    "    fig.add_subplot(1, 10, ii+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(disp_img[ii, :, :]), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define a Generator (G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator architecture\n",
    "<img src=\"./imgs/networkG.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 4*4*128)\n",
    "        self.deconv1 = nn.Sequential(nn.ConvTranspose2d(128, 64, 3, stride = 2, padding = 1),\n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.ReLU())\n",
    "        self.deconv2 = nn.Sequential(nn.ConvTranspose2d(64, 32, 4, stride = 2, padding = 1),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.ReLU())\n",
    "        self.deconv3 = nn.Sequential(nn.ConvTranspose2d(32, 1, 4, stride = 2, padding = 1),\n",
    "                                     nn.Tanh())\n",
    "                       \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.shape[0], 128, 4, 4)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        return x\n",
    "    \n",
    "'''\n",
    "Bring the Generator output from [-1, +1] -> [0, 1]\n",
    "'''\n",
    "def out2img(x):\n",
    "    return (x + 1.0)/2.0\n",
    "\n",
    "'''\n",
    "Preprocess the real image to input to the discriminator [-1, +1]\n",
    "'''\n",
    "def img2inp(x):\n",
    "    return (x - 0.5)*2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sample image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_size = 100 # Size of the random input vector\n",
    "G = Generator(z_size)\n",
    "\n",
    "# Generate a random vector and pass it through the generator.\n",
    "z = torch.randn(1, z_size)\n",
    "g_im = G(z)\n",
    "\n",
    "# Display.\n",
    "disp_img = out2img(g_im.detach().numpy().squeeze())\n",
    "print(\"Image size: {}x{}\".format(disp_img.shape[0], disp_img.shape[1]))\n",
    "plt.figure(figsize=(2, 2))  \n",
    "plt.axis('off')\n",
    "plt.imshow(disp_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define a Descriminator (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/networkD.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 4, stride = 2, padding=1),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 4, stride = 2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, stride = 2, padding=1),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "        self.fc = nn.Linear(128*4*4, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is not availabe, using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available(): \n",
    "    use_cuda = True\n",
    "    print(\"Using the GPU.\")\n",
    "else:\n",
    "    print(\"Cuda is not availabe, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(pred):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    expected_output = torch.ones(pred.shape[0], 1)\n",
    "    if use_cuda:\n",
    "        expected_output = expected_output.cuda()\n",
    "    return criterion(pred, expected_output)\n",
    "\n",
    "def fake_loss(pred):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    expected_output = torch.zeros(pred.shape[0], 1)\n",
    "    if use_cuda:\n",
    "        expected_output = expected_output.cuda()\n",
    "    return criterion(pred, expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate num_imgs x h x w tensor into a single image.\n",
    "def concat_imgs(np_imgs): \n",
    "    num_imgs = np_imgs.shape[0]\n",
    "    h = np_imgs.shape[1]\n",
    "    w = np_imgs.shape[2]\n",
    "    out = np.zeros((h, num_imgs*w))\n",
    "    for i in range(num_imgs):\n",
    "        out[:, i*w:(i+1)*w] = np_imgs[i,:,:]\n",
    "    return out        \n",
    "\n",
    "\n",
    "def train_GAN(G, D, optimizer_g, optimizer_d, input_size, batch_size = 32, num_epochs = 30):\n",
    "    sample_size = 10\n",
    "    z_fixed = torch.randn(sample_size, input_size)\n",
    "    if use_cuda:\n",
    "        z_fixed = z_fixed.cuda()\n",
    "        \n",
    "    # GET THE DATA\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = batch_size, shuffle= True)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        d_losses = 0.0\n",
    "        g_losses = 0.0        \n",
    "        # Iterate over the dataset.\n",
    "        for ii, (real_imgs, _) in enumerate(data_loader):\n",
    "            # Bring the input image pixels between -1 and 1\n",
    "            real_imgs = img2inp(real_imgs)   \n",
    "            if use_cuda:\n",
    "                real_imgs = real_imgs.cuda()\n",
    "                \n",
    "            # ======== TRAIN DESCRIMNATOR ===============\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            pred = D(real_imgs)\n",
    "            # D should learns to predict real image as real, i.e., label 1\n",
    "            real_loss_d = real_loss(pred)\n",
    "            \n",
    "            # Create fake examples.\n",
    "            z = torch.randn(batch_size, input_size)\n",
    "            if use_cuda:\n",
    "                z = z.cuda()\n",
    "            fake_imgs = G(z)    \n",
    "            pred = D(fake_imgs.detach())\n",
    "            # D should learns to predict the generated images as fake, i.e., label 0\n",
    "            fake_loss_d = fake_loss(pred) \n",
    "            \n",
    "            # Descriminator opt takes a step.\n",
    "            total_loss_d = real_loss_d + fake_loss_d\n",
    "            total_loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # ============= TRAIN GENERATOR =============\n",
    "            optimizer_g.zero_grad()\n",
    "            # Create more fake examples.\n",
    "            z = torch.randn(batch_size, input_size) \n",
    "            if use_cuda:\n",
    "                z = z.cuda()\n",
    "            fake_imgs = G(z)\n",
    "            pred = D(fake_imgs)     \n",
    "            # G tries to learn such that generated images pass as real.\n",
    "            loss_g = real_loss(pred) \n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            # Save losses\n",
    "            d_losses += total_loss_d.detach().cpu().item()\n",
    "            g_losses += loss_g.detach().cpu().item()\n",
    "            \n",
    "            sys.stdout.write(\"\\r Processing: {}/{}...\".format(ii+1, len(data_loader)))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        d_loss_epoch = d_losses/len(data_loader)\n",
    "        g_loss_epoch = g_losses/len(data_loader)\n",
    "        print(\"Epoch: {}, D loss: {}, G loss: {}\".format(e, d_loss_epoch, g_loss_epoch))\n",
    "        \n",
    "        # Save a few images as example.\n",
    "        G.eval()\n",
    "        out_imgs = G(z_fixed)\n",
    "        out_imgs = out2img(out_imgs) # Convert to right size.\n",
    "        disp_imgs = np.squeeze(out_imgs.detach().cpu().numpy())\n",
    "        combined_img = concat_imgs(disp_imgs)*255\n",
    "        combined_img = combined_img.astype(np.uint8)\n",
    "        img_to_save = Image.fromarray(combined_img)\n",
    "        img_to_save.save(\"./data/sample_output{}.png\".format(e))\n",
    "        \n",
    "        # Save the checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': e + 1,\n",
    "            'state_dict': G.state_dict()         \n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoint.pth')\n",
    "        G.train()\n",
    "    print(\"Done training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from file...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "G = Generator(z_size)\n",
    "D = Discriminator()\n",
    "if use_cuda:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n",
    "\n",
    "if os.path.exists('checkpoint.pth'):\n",
    "    print(\"Loading trained model from file...\")\n",
    "    if use_cuda:\n",
    "        G.load_state_dict(torch.load('checkpoint.pth')[\"state_dict\"])        \n",
    "    else:\n",
    "        G.load_state_dict(torch.load('checkpoint.pth', map_location='cpu')[\"state_dict\"])\n",
    "else:\n",
    "    print(\"Training...\")\n",
    "    # Train your own model.\n",
    "    lr = 0.0001\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "\n",
    "    optimg = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimd = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    train_GAN(G, D, optimg, optimd, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample image:  epoch 01\n",
    "<img src=\"./imgs/sample_output0.png\" width=\"800\">\n",
    "\n",
    "#### Sample images: epoch 30\n",
    "<img src=\"./imgs/sample_output29.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACOCAYAAADn/TAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAABQ5JREFUeJzt3E0opW0cx3HHy+R9ITSRWIsNko2t1CyoKRZSko2xUEozOxQWliwQWVEysrGwsJomrCymkZSNhZfGQs2EGMN5Nk8n///zOPjN8XLO+X5W92/O4dzpN7fLdV/XHQgGgwnAYyW+9AkgOlEcSCgOJBQHEooDCcWBhOJAQnEgSX7ODwsEAsw2RplgMBj4v3/nigMJxYGE4kBCcSChOJBQHEgoDiQUBxKKAwnFgYTiQPKs96riVSBgb/fEwgYBrjiQUBxIKA4kjHEioKyszOSNjQ2TMzIyTG5oaDB5eXn5aU7sCXHFgYTiQMKvKlFeXl7o+MuXL+a19PR0k6+urkx+9+6dySsrKyb/+fMnEqf4pLjiQEJxIKE4kDDGeaD8/HyT19fXQ8dpaWnmtQ8fPpi8tLRkcmFhocmJidH3/zf6zhivAsWBhOJAEnjOW/zRtAXYj1v29/dNvj3XUllZGfa90YwtwIgoigMJxYGEeZx/+aUP/f39JmdlZZnc09MTOj44OHiy83qtuOJAQnEgoTiQxO0Yx29ZaWlpMbm9vd3ktbU1k+fm5kLHKSkp5rXfv39H4hRfNa44kFAcSCgOJHE7xklNTTW5u7vbZD9vMzExYfLp6Wno2K8pfix/X2x7e9vk4eFhk6empv7q8yKBKw4kFAcSigNJ3K7H8Wtovn79avLe3p7JVVVVJp+fn8uf7eeQ/PqdgoICk/28UE5OjslnZ2fyudyH9TiIKIoDCcWBJG7mcfzeJb/3yfNzJRcXF/JnJyfbH/Pu7q7Jfs+W/yw/hikqKjJ5Z2dHPjcVVxxIKA4kFAeSuJnH8XMjm5ubYd9fUVFh8tHR0Z3v9WOY2tpak2dnZ01++/atyX7NclNTk8l9fX0mf/v2zeRPnz7deW5/i3kcRBTFgSRu/hz3j0/z0/b+USThfjV5ra2tJk9OTprsf5UtLi6a3NzcbLIfPnz8+NHkurq6B5/bU+GKAwnFgYTiQBI3Y5yOjg6Tk5KSTB4cHHzU97v9OLbe3t6w7/3+/bvJ941pPL+Ew2/HeQlccSChOJBQHEhidozjl2fm5uaafHl5afLJyUnY7+eXZSwsLISOi4uLzWt+mYO/fXHfmMaf+/j4uMnT09Nhv/45cMWBhOJAQnEgidkxjt/Cm5mZabIf07x58ybs9/NfX15eHjr245/R0VGT73vkfltbm8ldXV0m//z502R/r+slcMWBhOJAQnEgidkxjn+Mid9y4ud1/DjDz53U1NSYfPv+0a9fv8xrW1tbJjc2Npo8NDRkst/u4rfP+PU319fXCS+NKw4kFAcSigNJzG6P8fd75ufnTX7//r3Jfs2L3z5zeHhocn19fej49mPdEhL+O+fjHR8fmzwwMGDy58+fTX7JMQ3bYxBRFAcSigNJzI5xvNLSUpP9I/azs7NN9veXbm5u7sz+Z/jjxw+TV1dXTe7s7DTZrw16TRjjIKIoDiQUB5K4GeN4JSUlJo+NjZlcXV1tsh/jjIyMhI5nZmbMa379TDRjjIOIojiQUBxI4naMg4dhjIOIojiQUBxIKA4kFAcSigMJxYGE4kBCcSChOJBQHEgoDiQUBxKKAwnFgYTiQEJxIKE4kFAcSCgOJBQHEooDybNuj0Hs4IoDCcWBhOJAQnEgoTiQUBxIKA4kFAcSigMJxYGE4kBCcSChOJBQHEgoDiQUBxKKAwnFgYTiQEJxIKE4kFAcSCgOJP8AgJhCxd/wGeYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G.eval()\n",
    "z = torch.randn(1, z_size)\n",
    "if use_cuda:\n",
    "    z = z.cuda()\n",
    "G_out = G(z)\n",
    "img = np.squeeze(G_out.detach().cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(2, 2))  \n",
    "plt.axis('off')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Wonderful animations of convolution arithmatic: https://github.com/vdumoulin/conv_arithmetic\n",
    "2. A more detailed GAN example from torch developes: https://github.com/pytorch/examples/tree/master/dcgan\n",
    "3. How to train a GAN: tips and tricks: https://github.com/soumith/ganhacks\n",
    "4. NN_SVG- An online tool for generating network diagrams: http://alexlenail.me/NN-SVG/AlexNet.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
